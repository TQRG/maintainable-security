\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{framed}
\usepackage{float}
\usepackage{xcolor}
\definecolor{mypink3}{cmyk}{0, 0.7808, 0.4429, 0.1412}



\usepackage{listings}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{lightgreen}{rgb}{0.6,0.9,0.6}
\definecolor{lightyellow}{rgb}{0.9,0.9,0.6}
\definecolor{lightorange}{rgb}{0.9,0.8,0.6}
\definecolor{lightred}{rgb}{0.9,0.7,0.7}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{lightgray}{rgb}{0.8,0.8,0.8}
\definecolor{mymauve}{rgb}{0.58,0,0.82}


\usepackage{pifont}
\newcommand{\lstbg}[3][0pt]{{\fboxsep#1\colorbox{#2}{\strut #3}}}
\lstset{ %
  language=C,
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\ttfamily\scriptsize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygray}\bfseries,    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
  frame=single,
  numbers=left,
  stepnumber=1,
  xleftmargin=2em,
  escapeinside={/*!}{!*/},
  moredelim=**[l][\color{mygreen}]{+},
  moredelim=*[l][\color{red}]{-}
}


\newcounter{lstannotation}
\setcounter{lstannotation}{0}
\renewcommand{\thelstannotation}{\ding{\number\numexpr181+\arabic{lstannotation}}}
\newcommand{\annotation}[1]{\refstepcounter{lstannotation}\label{#1}\thelstannotation}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\Sof}[1]{\textbf{[Sofia:[}{\color{red} #1}\textbf{]]}}


\begin{document}

\title{Security vs. Maintainability: Fixing Vulnerabilities Obfuscates your Code}

\author{
    Anonymou(s) Author(s)
%     \IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address}
}

\maketitle

\begin{abstract}
  Security is a crucial non-functionality requirement for software applications.
  However, building secure software is far from trivial as developers lack both
  the knowledge and tools to effectively address this concern. In this paper, we
  study the impact of changes to improve security on the maintainability of several
  open source applications. Using a dataset containing 607 security-oriented
  commits, we measure maintainability --- as computed by the Software Improvement
  Group's web-based source code analysis service \emph{Better Code Hub} (BCH) ---
  before and after the security refactoring. Results show that making software
  more secure comes at a cost on maintainability. This is particularly evident
  in refactorings to deal with \textit{Broken Authentication} and \textit{Cross-Site Request Forgery} attacks.
\end{abstract}

\begin{IEEEkeywords}
Security, Software Maintenance, Open-Source Software
\end{IEEEkeywords}

\section{Introduction}

Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. 

The international standard ISO/IEC 25010:2011 breaks down software quality into eight characteristics: maintainability,
functional suitability, performance efficiency, compatibility, usability, reliability, security,
and portability. This paper focuses solely on maintainability.

\Sof{=============== READY START!}

This paper is structured as follows: section \ref{sec:motivation} introduces an example of a security refactoring of a known vulnerability found in OpenSSL\footnote{Repository details available at https://github.com/openssl/openssl (Accessed on January 27, 2019)} software; section \ref{sec:methodology} describes the methodology used to answer the research questions; section \ref{sec:results} presents the results and section \ref{sec:discussion} discusses their implications; section \ref{sec:threats}, threats to the validity of the study are enumerated; section \ref{sec:rw} describres the different work and existing literature in the field of study; and, finally section \ref{sec:conclusions} concludes the main findings and elaborates on future work.


\section{Motivation and Research Questions}\label{sec:motivation}


\Sof{@todo: Improve this paragraph}Under the rush of companies trying to outdo each other and due to the lack of expertise in the security field, security flaws are usually detected only when hackers exploit them which happens
because developers create buggy software without knowing. Gladly, fixing these
issues usually is as simple as changing the software codebase. However,
these refactorings can sometimes have a negative impact on software maintenance, not
only because developers are under the rush of "protecting" their software but also because 
they tend to take the easiest path and not the most efficient one. Thus, we
want to understand how security refactorings impact the maintainability of software.

One example is the refactoring of the DTLS protocol implementation in OpenSSL after
Adam Langley detect that remote attackers could cause a Denial-of-Service (DoS) attack
when crafting DTLS handshake messages to trigger memory allocations corresponding to large length values. 
This vulnerability is listed at the Common Vulnerabilities and Exposures dictionary as CVE-2014-3506\footnote{CVE-2014-3506 details available at http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-3506 (Accessed on January 27, 2019)} and 
it is one of the vulnerabilities under evaluation in this study. The impact of this refactoring
on the OpenSSL maintainability was negative. The following snippet presents the codebase changes 
performend on the \emph{ssl/d1\_both.c} file\footnote{CVE-2014-3506 fix available  at https://github.com/openssl/openssl/commit/\\1250f12613b61758675848f6600ebd914ccd7636 (Accessed on January 27, 2019)} by the OpenSLL developers to fix the vulnerability.


\setcounter{lstannotation}{0}
\begin{lstlisting}[caption={Fix provided by OpenSSL developers to the CVE-2014-3506 vulnerability \Sof{@todo: Fix the regex issue}},label={lst:vuln}]
+static unsigned long dtls1_max_handshake_message_len(const SSL *s){ /*!\annotation{lst:func1}!*/
+
+ unsigned long max_len = DTLS1_HM_HEADER_LENGTH + SSL3_RT_MAX_ENCRYPTED_LENGTH;
+ if(max_len < (unsigned long)s->max_cert_list) /*!\annotation{lst:func2}!*/
+    return s->max_cert_list;
+ return max_len;
+} 

static int dtls1_reassemble_fragment(SSL *s, struct hm_header_st* msg_hdr, int *ok){
    
// [snip]

- unsigned long frag_len = msg_hdr->frag_len, max_len; /*!\annotation{lst:func3}!*/
-
- if((msg_hdr->frag_off+frag_len) > msg_hdr->msg_len) 
-    goto err; 

- if(DTLS1_HM_HEADER_LENGTH + SSL3_RT_MAX_ENCRYPTED_LENGTH < s->max_cert_list)
-    max_len = s->max_cert_list;
- else
-    max_len = DTLS1_HM_HEADER_LENGTH + SSL3_RT_MAX_ENCRYPTED_LENGTH;

+ unsigned long frag_len = msg_hdr->frag_len;
 
- if((msg_hdr->frag_off+frag_len) > max_len)

+ if((msg_hdr->frag_off+frag_len) > msg_hdr->msg_len ||
+ msg_hdr->msg_len > dtls1_max_handshake_message_len(s))
     goto err; /*!\annotation{lst:func5}!*/
	 
  memset(seq64be,0,sizeof(seq64be));
  seq64be[6] = (unsigned char) (msg_hdr->seq>>8);
  seq64be[7] = (unsigned char) msg_hdr->seq;
  item = pqueue_find(s->d1->buffered_messages, seq64be);
  if(item == NULL)
  {
    frag = dtls1_hm_fragment_new(msg_hdr->msg_len, 1);
		
// [snip]
 
 static int
 dtls1_process_out_of_seq_message(SSL *s, struct hm_header_st* msg_hdr, int *ok)
 {
 
// [snip]

  if(frag_len && frag_len < msg_hdr->msg_len)
     return dtls1_reassemble_fragment(s, msg_hdr, ok);
 
+ if(frag_len > dtls1_max_handshake_message_len(s))
+    goto err; /*!\annotation{lst:func4}!*/
+
  frag = dtls1_hm_fragment_new(frag_len, 0);

\end{lstlisting}

In the \texttt{dtls1\_reassemble\_fragment} function the value of \texttt{msg\_hdr-$>$frag\_off+frag\_len} was checked against the
maximum handshake message size but \texttt{msg\_len} is allocated to the fragment buffer in line 37. Thus, if the fragment was within
the allowed size, the pending handshake message could consume $16MB$+$2MB$. According with the message commit, 10 handshake messages
are allowed, so the attacker could consume approximately $180MB$ per DTLS connection.

The code changes performed to fix the vulnerability were the following:

\ref{lst:func1} \texttt{dtls1\_max\_handshake\_message\_len} function was inserted to limit the number of bytes allowed in a DTLS handshake message for \texttt{s}. This function replaces the lines 18-21.

\ref{lst:func2} Checks if the maximum certificate list size (\texttt{s-$>$max\_cert\_list}) requires more length than the maximum length (\texttt{max\_len}). If yes, it return the certificate list size, if not it return the maximum length.

\ref{lst:func3} Replaced by attribution in line 23.

\ref{lst:func5} and \ref{lst:func4} add the check to ensure that the message length in never higher than the maxiumum allowed by the DTSL protocol in different functions.

Although limiting the size of a handshake message seems an elementary problem to repair, the example shows that there was a significant amount of changes performed in the codebase which produced a negative impact in the project maintainability. Even though the developer tries to simplify the codebase with the creation of a smaller unit \texttt{dtls1\_max\_handshake\_message\_len}, it also disrupts 2 of the guidelines proposed by the Software Improvement Group (SIG) for building maintainable software~\cite{Visser:2016:OREILLY}. It adds a new branch point to the \texttt{dtls1\_process\_out\_of\_seq\_message} unit (\ref{lst:func5}) increasing the cyclomatic complexity and breaking the \emph{Write Simple Units of Code} guideline. And, it increases the codebase size in $4$ lines of code ($13$ lines added, $9$ lines deleted) which infringes the \emph{Keep Your Codebase Small} guideline. 

In this study, our concern is if while improving software security, developers are also inserting a negative impact on the maintainability of their software which may increase technical debt when choosing the easiest solution instead of the using the best approach. To answer the following two research questions, we use a dataset of security refactorings to measure the impact of the changes in the maintainability of open-source software.


\begin{framed}
\textit{\textbf{RQ1} What is the impact of security refactorings on the maintainability of open-source software?}
\end{framed}

Often security flaws require refactoring code to make software more secure. However, there is no evidence yet of how security refactorings impact the maintainability of open-source software. Our suspicion is that developers tend to introduce technical debt in their software when refactoring the codebose to address a security flaw because they tend to choose the easiest path to solve it. To address it, we compute the maintainability value of $607$ commits using \emph{Better Code Hub}. The same approach is applied to a randomly generated dataset of regular commits - baseline - that we use to understand how maintainability evolves when security refactorings are performed versus when they are not.

\begin{framed}
\textit{\textbf{RQ2} Which patterns of security refactorings are more likely to affect open-source software maintainability?}
\end{framed}

There are security flaws that are more difficult to refactor than others. For instance, introducing a mechanism to secure authentication is not so easy to implement as fixing a cross-site scripting vulnerability with a \texttt{escape} function. Knowing which patterns are more likely to increase maintainability issues is one step forward to bring awareness to security engineers of what patterns need more attention. The taxonomy of security patterns used to answer this question is the same as the one provided by the authors of the dataset. Maintainability is measured separately for each pattern.


\section{Methodology}\label{sec:methodology}

In this section, the methodology used to measure the impact of security refactorings on the maintainability of open-source software is presented in depth in the following sections and illustrated in Figure \ref{fig:met}. Aiming to answer the research questions presented in this paper, we use a dataset containing $716$ security refactorings collected from open-source software available on GitHub. $648$ of those refactorings were reported in a previous study conducted in 2017 \cite{Reis:2017:IJSSE}. In parallel, a baseline of regular commits was randomly collected from the list of projects of the main dataset to evaluate what is the impact of regular commits on the maintainability of open-source software. The Software Improvement Group's\footnote{SIG's website is available at https://www.sig.eu/ (Accessed on January 23, 2019)} web-based source code analysis service Better Code Hub (BCH) \footnote{BCH's website is available at https://bettercodehub.com/ (Accessed on January 23, 2019)} was used to collect the maintainability reports for both security and regular commits.


\begin{figure}[h]
 	\centering 	\includegraphics[width=0.5\textwidth]{figures/methodology.pdf}
 	\caption{Study Methodology \Sof{@todo: Improve it!}}
	\label{fig:met}
\end{figure}

\subsection{Dataset}

To evaluate the impact of security refactorings on the maintainability of open-source software, we use a dataset of security flaws which is the outcome of mining more than $248$ GitHub\footnote{Github's website available at https://github.com/ (Accessed on January 23, 2019)} projects. Reis and Abreu (2017) mined open-source software aiming the extraction of real test cases - created by real developers on their daily basis development - to test and assess the performance of static analysis tools since using hand-seeded test cases or mutations could lead to misleading assessments of the capabilities of the tools. The study yielded to a dataset of $648$ test cases for $16$ different patterns. Each test case of the dataset is a triplet of folders: the commit before the refactoring, the commit responsible for the refactoring and the snippets of code that differ from one version to another (usually, called \textit{diff}) - where one can easily identify the code used to fix the security flaw. In this study, we focus on computing the maintainability of the commits before and after the security refactoring to evaluate if the impact was positive, negative or none.

\begin{figure}[h]
 	\centering 	\includegraphics[width=0.5\textwidth]{figures/language_dist.pdf}
 	\caption{Security Refactorings Language Distribution}
	\label{fig:lang}
\end{figure}

\begin{figure}[h]
 	\centering 	\includegraphics[width=0.5\textwidth]{figures/type_dist.pdf}
 	\caption{Projects Domain Distribution}
	\label{fig:domain}
\end{figure}

Reis and Abreu ($2017$) dataset commits were manually validated, flagged as security flaws and classified with one of the $16$ following patterns: \textit{Injection}, \textit{Broken Authentication and Session Management}, \textit{Cross-Site Scripting}, \textit{Broken Access Control}, \textit{Security Misconfiguration}, \textit{Sensitive Data Exposure}, \textit{Insufficient Attack Protection}, \textit{Cross-Site Request Forgery}, \textit{Using Components With Known Vulnerabilities}, \textit{Unprotected APIs}, \textit{Memory Leak}, \textit{Overflow}, \textit{Denial-of-Service}, \textit{Path Traversal} and \textit{Miscellaneous}. However, the dataset evolved and now integrates new patterns, such as \textit{Resource Leak} and \textit{SHA-1 Hash Function}. A considerable amount of these patterns are based in the OWASP Top 10 of 2013~\cite{oswap:2013} and OWASP Top 10 of 2017~\cite{oswap:2017}.

In total, the dataset contains $716$ security refactorings. These refactorings were computed by the BCH tool to calculate maintainability. However, some of the refactorings are not considered in the final results due to limitations of BCH: lack of language support and project size. The final dataset contains $607$ security refactorings. The Wilcoxon statistical test - described more in-depth in subsection \ref{sec:statsval} - is performed to evaluate the significance of our results. Each type of security flaw is tested, Figure \ref{fig:pat}. One of the main requirements of this test is that the input sample needs to comprise more than $20$ commits. Thus, the patterns not complying with this requirement were reallocated to the \textit{Miscellaneous} pattern. In the end, we highlight the following patterns:

\begin{itemize}
	\item \textbf{Using Components with Known Vulnerabilities.} The majority of software produced today integrates several components, such as libraries and frameworks, which may affect the software if developers use vulnerable versions - usually known and disclosed somewhere on the Internet. \textit{Based on the OWASP Top 10 of 2017.}
	\item \textbf{Broken Authentication \& Session Management.} Uncorrectly implemented functionalities related to authentication and session management, allowing crackers to gain access to session tokens, passwords, keys and other sensitive data. \textit{Based on the OWASP Top 10 of 2013.}
	\item \textbf{Cross-Site Request Forgery.} Poor session tokens generation and management usually allow crackers to send forged HTTP requests including authentication information from the victim to vulnerable web application. \textit{Based on the OWASP Top 10 of 2013.}
	\item \textbf{Denial-of-Service} Security flaws that can allow a cracker to flood or crashing services. This attacks usually occur when the system receives much traffic causing it to slow down and eventually stop. For example, flaws allowing crackers to trigger memory allocations corresponding to large length values (Listing \ref{lst:vuln}).
	\item \textbf{Miscellaneous} This pattern comprises several others categories of security refactorings (e.g., \textit{path traversal and buffer overflow}). It contains several other security refactorings that do not have their own pattern yet. The patterns not satisfying the size requirement of more than 20 refactorings are contemplated here.
	\item \textbf{Cross-Site Scripting.} Lack of proper validation or escaping allow crackers to submit untrusted data to web browsers through malicious scripts that can hijack the user sessions or redirect the user to malicious sites. \textit{Based on the OWASP Top 10 of 2017.}
	\item \textbf{Injection.} When developers do not keep untrusted data separate from commands and queries. If a cracker sends a string that exploits the syntax of the interpreter, then an injection attack is possible (e.g., SQL and LDAP injection). \textit{Based on the OWASP Top 10 of 2017.}
	\item \textbf{Memory Leak.} Memory management issue found more frequently in programming languages that do not manage memory automatically (e.g., C/C++ and Objective-C), i.e., where instead developers are responsible for handling it. One of the main causes of DoS attacks.
\end{itemize}

The dataset contemplates security flaws for more than 13 different languages being PHP (39\%) and C (24\%) the most prevalent ones, Figure \ref{fig:lang}.  To classify the software of the dataset, we use a taxonomy for open-source software published in an older study~\cite{7816479}. Figure \ref{fig:domain} presents the projects domain distribution: \textit{Aplication Software} ($25$), software that provides end-users with functional systems; \textit{Web libraries and frameworks} ($20$); \textit{System Software} ($18$), software that provides services and infrustructures (e.g., operating systems, servers and databases); \textit{Software Tool} ($13$), software that supports development (e.g., programming languages, compilers, package managers, IDEs); and, \textit{Non-web libraries and frameworks} ($18$), for desktop and mobile software.

\begin{table*}[h]
	\centering
	\caption{Descriptive statistics of the dataset projects}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
      & forks   & stars   & watchers & contributors & commits  & branches & releases & size      & issues & pull requests  \\ \midrule
mean  & 1763.52 & 5448.74 & 401.37   & 153.33       & 14834.17 & 45.17    & 129.45   & 122973.24 & 3768.97   & 1941.61 \\
std   & 2434.03 & 6215.09 & 486.60   & 123.68       & 22234.46 & 150.15   & 189.93   & 209732.51 & 5933.16   & 3603.31 \\
min   & 1       & 3       & 1        & 0            & 103      & 1        & 0        & 108       & 0         & 0       \\
25\%  & 391.50  & 1581    & 117.25   & 49           & 1440.50  & 4        & 19       & 8466.75   & 313.75    & 143.25  \\
median  & 838.50  & 2836.50 & 248      & 99           & 5504.50  & 9        & 59       & 37372.50  & 1792.50   & 650     \\
75\%  & 2155    & 6828.50 & 459.50   & 261          & 18579.25 & 20       & 142.75   & 117699.50 & 4087.75   & 1907.25 \\
max   & 16366   & 31841   & 3446     & 413          & 114378   & 1227     & 1114     & 995790    & 33970     & 19329   \\
Total & 165771  & 512182  & 37729    & 14413        & 1394412  & 4246     & 12168    & 11559485  & 354283    & 182511  \\ \bottomrule
\end{tabular}
\label{tab:dataset}
\end{table*}

Table \ref{tab:dataset} presents the descriptive statistics of the 94 open-source projects involved in this study. \Sof{@all: Should I expand this? I am not sure what it is important to say about this.}

\subsection{Security vs. Baseline Commits}

Previous studies tried to measure the impact of refactorings on open-source software maintainability~\cite{HEGEDUS2018313} before. But to the best of our knowledge, none of them used the BCH model and tried to evaluate the impact of security refactorings compared with regular refactorings as we do. Thus, we analyze the maintainability of regular commits and use them as a baseline.

The baseline dataset uses the security commits dataset as input. For each
security commit, one random commit is selected from the list of all commits of
the belonging project. We originate the regular commits from the security
commits to ensure that differences in maintainability are not consequence of
characteristics of different projects.

\subsection{Maintainability Analysis}

Better Code Hub is used to collect the maintanability reports of the refactorings of each project. Figure \ref{fig:guidelines} presents the 10 different guidelines proposed by BCH authors for delivering software that is easy to maintain \cite{Visser:2016:OREILLY}. The reports provided by the tool are based on the following guidelines:

\begin{figure}[H]
 	\centering 	\includegraphics[width=0.49\textwidth]{figures/guidelines.pdf}
 	\caption{Guidelines to produce maintainable code}
	\label{fig:guidelines}
\end{figure}

All guidelines are measured in the same evaluation by BCH. During each guideline evaluation, the tool determines the compliance towards one guideline by establishing limits for the percentage of code allowed to be in each of the 4 risk severity levels (\emph{low risk}, \emph{medium risk}, \emph{high risk} and \emph{very high risk}). If the project does not violate the thresholds, then it is compliant with the guideline. These thresholds are calibrated by BCH using their own data - open-source and closed software systems. If a project is compliant with a guideline, it means that it is at least $65\%$ better than the software used by BCH to calculate the thresholds\footnote{Check the answer to \emph{How can I adjust the threshold for passing/not passing a guideline?} at https://bettercodehub.com/docs/faq (Accessed on January 27, 2019)}. Figure \ref{fig:bchrep} shows an example of the  guidelines report provided by BCH for a project after finishing its evaluation. These are the results after performing the OpenSSL CVE-2014-3506 vulnerability refactoring - motivation example discussed, previously, in section \ref{sec:motivation}. This version of OpenSSL only complies with 1 out of 10 guidelines, the \emph{Write Clean Code} guideline.

\begin{figure}[H]
 	\centering 	\includegraphics[width=0.45\textwidth]{figures/bch_report.png}
 	\caption{Maintainability report of OpenSSL CVE-2014-3506 vulnerability refactoring for the guideline \emph{Write Simple Units of Code} provided by \emph{Better Code Hub}. This version of OpenSSL does not comply with the guideline in the example since the bars do not reach the threshold points. This example only complies with 1/10 guidelines (\emph{Write Clean Code}).}
	\label{fig:bchrep}
\end{figure}

\emph{Units} are the smallest groups of code that can be maintained and executed independently \cite{Visser:2016:OREILLY}. McCabe \cite{1702388} is a software metric that calculates the number of linearly independent paths of the source code. BCH uses this metric to measure the number of branch points in the \emph{Write Simple Units of Code} guideline. The bar represents the top 30 of units that violates the guideline, sorted by severity, which is indicated by the colors of the checkboxes. The green bar represents the number of compliant branch points per unit (\emph{at most 5}), i.e., the units are compliant with ISO 25010 \cite{iso:2011}. Yellow, orange and red bars represent units that do not comply with medium (\emph{above 5}), high (\emph{above 10}) and very high (\emph{above 25}) severity levels. In the bar, there are marks that pinpoint the compliance thresholds for each severity level.

\begin{figure}[H]
 	\centering 	\includegraphics[width=0.49\textwidth]{figures/commit.pdf}
 	\caption{Maintainability difference for security commits}
	\label{fig:commit}
\end{figure}

Aiming to analyze the impact of security refactorings, we use BCH to compute maintainability of two different versions of the project (Figure \ref{fig:commit}):
\begin{itemize}
	\item $v_{s-1}$, the version containing the security flaw, i.e., before the refactoring;
	\item $v_{s}$, the version after fixing the security flaw, i.e., after the refactoring;
\end{itemize}

The BCH tool does not compute the final score that our study needs to compare maintainability amongst different project versions. Thus, our work proposes an equation to capture the distance between the current state of the project and the standard thresholds based on previous work \cite{Olivari:2018}. 
\begin{itemize}
	\item \textbf{Size project changes do not affect the maintainability difference between two versions, $\Delta M (v_{s-1},v_{s})$.} In this work, we aim to evaluate the alike security patterns occurring in different projects. Thus, normalization based on its size cannot be used - we convert relative data to the respective number of lines of code.
	\item \textbf{Distance to lower severity levels are less penalized than to the thresholds in high severity levels.} Severity level weights based on the severity level to count lines of code that violate maintainability guidelines.
\end{itemize}

We compute the mean average of the maintainability score $M(v)$ for all the selected guidelines, as follows:

\begin{equation}
    M(v) = \sum_{g \in G}^{} M_{g}(v)
\end{equation}

\noindent
where $G$ is the group of selected maintainability guidelines from BCH (Figure \ref{fig:guidelines}) and $v$ is the
version of the software under evaluation. The maintenance $M$ based on the guideline $g$ for a given version of a project is computed with the following equation:

\begin{equation}
    M_{g} = \frac{1}{|L|} \sum_{l \in L}^{} C(l) , L = \{medium, high, veryHigh\}
\end{equation}

\noindent
where $C$ is the compliance with the maintainability guideline for the given severity level (medium, high, and very high) and
$L$ is the group of severity levels of maintainability infractions. The compliance $C$ for a given severity level $l$ is derived by:

\begin{equation}\label{eq:3}
    C(l) = LOC_{compliant}(l) - w(l) * LOC_{\neg compliant}(l)
\end{equation}

\noindent
where $LOC_{compliant}(l)$ are the lines of code that comply with the guideline at the given severity level $l$, $LOC_{\neg compliant}(l)$ are the lines of code that do not comply with the guideline at the given
severity level $l$ and $w(l)$ is the weight factor to boost the impact of
non-compliant lines in comparison to compliant lines. Finally, the term $w(l)$ is calculated as follows:

\begin{equation}
    w(l) = \frac{1 - T(l)}{T(l)}
\end{equation}

\noindent
where $T(l)$ is the threshold in percentage of the lines of code that are accepted to be non-compliant with the guideline for the severity level $l$. This is a standard value defined by BCH. In other words, the factor $w$ is used in Equation \ref{eq:3} to highlight the lines of code that are not complying with the guideline. Then, we compute the difference of maintainability between the security commit ($v_{s}$) and its parent commit ($v_{s-1}$), as illustrated in Figure \ref{fig:commit}.

\subsection{Statistical Validation}\label{sec:statsval}

To validate the maintainability differences in different groups of commits (e.g., baseline and security commits), we use the Paired Wilcoxon signed-rank test \cite{10.2307/3001968} with the significance level $\alpha = 0.05$. In other words, we test the null hypothesis that the maintainability difference between pairs of versions $v_{v-1}$, $v_v$ (i.e., before and after a security-commit) follows a symmetric distribution around $0$. However, when the difference between groups is zero, the observations are discarded. Thus, we use a more robust approach of the wilcoxon test, the Wilcoxon test by Pratt which provides an alternative that incorporates the cases where the maintainability difference is zero \cite{10.2307/2282543}. To understand the effect-size, as advocated by the Common-language effect sizes \cite{graw:1992}, we compute the mean difference, the median of the difference, and the percentage of cases that reduce maintainability. 

\section{Results}\label{sec:results}

In total, this study evaluates $607$ security and baseline commits from 94 different open-source projects distributed among 5 different main types of software, Figure \ref{fig:domain}. The following section presents the results obtained for each research question.

\begin{framed}
\textit{\textbf{RQ1} What is the impact of security refactorings on the maintainability of open-source software?}
\end{framed}

The impact of security and baseline commits in the maintainability of open-source software is presented in Figure \ref{fig:secvsreg}. 

\begin{figure}[h]
 	\centering 	\includegraphics[width=0.5\textwidth]{figures/maintainability.pdf}
 	\caption{Maintainability difference for security and baseline refactorings}
	\label{fig:secvsreg}
\end{figure}




% The results on the impact of different categories of commits in software maintainability are present in the plot bar of Fig. 6. The plot presents the results for two groups of software changes: energy commits, and baseline commits. For each group, the figure provides three bars with the percentage of commits which 1) decrease maintainability (on top, colored in red), 2) do not change maintainability (in the middle, colored in yellow), and 3) increase maintainability (in the bottom, colored in green). In addition, the figure provides, for each group, the mean (x ̄) and the median (Md) of the maintainability difference, and the p-value of the Wilcoxon signed-rank test (p).
% In the case of the regular commits, used as a baseline, 33.0% decrease maintainability (183 cases), 38.7% do not change maintainability (215 cases), and 28.3% improve maintainabil- ity (157 cases). Since the p-value of the Wilcoxon signed-rank test (p = 0.139) is not below the significance level (α = 0.05), there is no statistical significance of the impact of regular commits on maintainability.
% On contrary, we observe clear changes for energy commits: 57.1% (310 cases) decrease software maintainability, 10.7% do not change maintainability (61 cases), and 31.2% improve maintainability (168 cases). The results for the Wilcoxon signed-rank test show statistical significance that energy com- mits decrease the maintainability of Android applications (p < 0.001).




\begin{framed}
\textit{\textbf{RQ2} Which patterns of security refactorings are more likely to affect open-source software maintainability?}
\end{framed}



\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.55\textwidth]{figures/category.pdf}
 	\caption{Maintainability difference by type after security refactorings}
	\label{fig:pat}
\end{figure}


\section{Discussion}\label{sec:discussion}


\begin{framed}
\textit{\textbf{RQ1} What is the impact of security refactorings on the maintainability of open-source software?}
\end{framed}


\begin{framed}
\textit{\textbf{RQ2} Which patterns of security refactorings are more likely to affect open-source software maintainability?}
\end{framed}



\section{Threats to Validity}\label{sec:threats}



\section{Related Work}\label{sec:rw}




\section{Conclusion and Future Work}\label{sec:conclusions}



\section*{Acknowledgment}\label{sec:ack}


We thank to the Software Improvement Group (SIG) team for all the support and help in validating our methodology.

{
 \bibliographystyle{IEEEtran}
  \bibliography{icpc19}
}

\end{document}

\documentclass[11pt,fleqn]{article}

\usepackage{vmargin}
\setpapersize{A4}
\setmarginsrb{2.5cm}{2.5cm}{2.5cm}{2.5cm}%
{\baselineskip}{2\baselineskip}{baselineskip}{2\baselineskip}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\usepackage{caption}
\usepackage{subcaption}


\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{url}
\usepackage{tgheros}
\renewcommand*\familydefault{\sfdefault}

\newcommand{\eline}{\vspace*{.75\baselineskip}}
\newcommand{\Referee}[1]{\eline \noindent {\bf Reviewer comment #1:} \\}
\newcommand{\Us}{\eline \noindent {\bf Response:}\\}
\newcommand{\TBD}{{\bf To Be Done}}
\newcommand{\newreviewer}[1]{\section*{Reviewer #1}\vspace*{-1.05\baselineskip}}
\newcommand{\editor}[1]{\section*{Editor #1}\vspace*{-1.05\baselineskip}}

%\usepackage{tikz}
\usepackage[skins,breakable]{tcolorbox}
\tcbset{textmarker/.style={
%
skin=enhancedmiddle jigsaw,breakable,parbox=false,before skip=-1mm, after skip=0mm,
boxrule=0mm,leftrule=2mm,rightrule=2mm,boxsep=0mm,arc=0mm,outer arc=0mm,
left=2mm,right=2mm,top=2mm,bottom=2mm,toptitle=1mm,bottomtitle=1mm,oversize}}

\newtcolorbox{rcomment}{textmarker,colback=gray!15!white,colframe=gray!80!white}

\newenvironment{revcomment}[1][]
{\Referee{#1}\begin{rcomment}}
{\end{rcomment}}

\newenvironment{reveditor}
{\begin{rcomment}}
{\end{rcomment}}

\usepackage{todo}
\usepackage{hyperref}

%customized
\usepackage{xfrac}

% more lenient line breaking (avoids text protruding into margins)
\sloppy

\title{\vspace*{-2cm}{\bf Authors' Response to the Review of
 EMSE-D-20-00300:\\
 ``Fixing Vulnerabilities Potentially Hinders Maintainability''}}

\author{Sofia Reis, Rui Abreu, Luis Cruz}
\date{}

\begin{document}

\maketitle

\editor{}

\begin{reveditor}
    Overall, I would like to give you a chance to respond to 
    these issues in a revision of your manuscript but I have 
    to make it clear that it does not guarantee a path towards 
    acceptance. Your rebuttal to \#1 will be of critical importance 
    for me judge the validity of the use of the BCH tool and in 
    making a final decision. Methodology is, of course, very 
    important for ESE journal and \#2 \& \#3s comments on methodological 
    details need to be carefully addressed, too. 
\end{reveditor}

\Us We thank the editor and reviewers for their valuable feedback. In the 
following points, we address the concerns raised by the reviewers.

\newreviewer{1}

\begin{revcomment}[1.1]

    This paper investigates the impact of patches to improve 
    security on the maintainability of open-source software. 
    The paper is well written and easy to read. I really 
    appreciated the replication package: complete, and well 
    described. Really a good work!
    
    However, I found a crucial issue in this paper. The authors 
    adopted as static analysis tool Better Code Hub. Better Code 
    Hub' model includes 10 guidelines that can help the developers to 
    write better code. Unfortunately, these guidelines are not related 
    to maintainability. Avoiding introducing the issues associated to 
    these guidelines do not imply increasing the maintainability of the 
    code, since the tools has no possibility to measure "maintainability".

\end{revcomment}

\Us Thank you for your positive feedback on the paper presentation and replication
package. BCH is not a static analysis tool instead it checks codebases for 
compliance with the ten guidelines. The set of guidelines presented in our paper
is the same published by Software Improvement Group (SIG) in their ebook called
"Building Maintainable Software: Ten Guidelines for Future-Proof Code"
\footnote{Available here: https://www.softwareimprovementgroup.com/resources/ebook-building-maintainable-software/}.
According to the authors, all the guidelines were infered from analyzing
hundres of real-world systems and measure maintainability.

We clarified the difference between BCH and static analysis tools in
the Section 1 (Introduction) and Section 3 (Methodology).

\begin{quote}
    Introduction\\
    ...\\
    Static analysis tools (SATs) have been built to automatically detect software 
    vulnerabilities (e.g., FindBugs, Infer and more). Developers
    use those tools to locate the issues in the code. However,
    while performing the patches to those issues, SATs are not 
    capable of providing information on the quality of the patch.
    Improving software security is not a trivial task and 
    requires implementing patches that might affect software 
    maintainability. Our hypothesis is that some of these patches may 
    have a negative impact on the software maintainability and, 
    possibly, even be the cause of the introduction of new 
    vulnerabilities---harming software reliability and introducing 
    technical debt. Research found that $34\%$ of the security patches 
    performed introduce new problems and $52\%$ are incomplete and do not 
    fully secure systems[Li2017]. Therefore, in this paper, 
    we present an empirical study on the impact of patches of 
    vulnerabilities on software maintenance across open-source software.
    We arguee that tools that assess these type of metrics may
    complement SATs with valuable information to help the developer
    understand better the risk of its patch.\\
    ...
\end{quote}

[Li2017] F. Li and V. Paxson. A Large-Scale Empirical Study of Security Patches. 
A Large-Scale Empirical Study of Security Patches in 2017.

\begin{revcomment}[1.2]

    As far as I know, the company that developed Better Code Hub 
    developed also the Delta Maintainability Model [diBiase2019] 
    that aims at measuring the maintainability of a code change 
    and a score and compare change-based maintainability measurements.

    [diBiase2019] M. di Biase and A. Rastogi and M. Bruntink and A. van 
    Deursen. The Delta Maintainability Model: Measuring Maintainability 
    of Fine-Grained Code Changes. IEEE/ACM International Conference 
    on Technical Debt (TechDebt) in 2019.

\end{revcomment}

\Us Thank you for suggesting Marco's paper. Our paper uses a model 
for measuring maintainability that was published previously by one of the co-authors
of this paper [Cruz2019]. The entire methodology and calculations 
were actually validated by Marco. 

The model created by Marco focus on measuring maintainability of fine-grained 
changes using 5 of the same guidelines we already use. The SIG-MM model in 
Marco's paper is the model behind Better Code Hub. 

We added Marco's work to the related work. 

[Cruz2019] Luis Cruz, Rui Abreu, John Grundy, Li Li, Xin Xia. 
Do Energy-oriented Changes Hinder Maintainability?. International 
Conference on Software Maintenance and Evolution (ICSME) in 2019.


\begin{revcomment}[1.3]

    Moreover, I am not sure that Better Code Hub is 
    the best static analysis tools to detect security 
    issues. One of the most adopted in security domain 
    is Coverity Scan for examples, but also other tools 
    might be a better choice (e.g. Kiuwan).

\end{revcomment}

\Us Thank your for your comment. In this paper, we do not
try to use static analysis to detect security issues.
We have already a dataset of security patches (vulnerable versions
and safe versions) and our goal is to understand what 
is the impact of those patches on the maintainability
guidelines/metrics. We arguee that these guidelines/metrics
can in the future complement static analysis tools 
by assiting developers with more information on the 
risks associated with their patches.

SonarQube uses 4 metrics of the set of metrics we are using and 
also rates maintainability. In Section 3 (\textit{Why Bettter Code Hub?}), 
we compare the results between BCH and SonarQube. However, 
we arguee that BCH performs a more complete analysis.



\begin{revcomment}[1.4]

    The authors could have adopted both approaches, Better 
    Code Hub guidelines or another one first and then measure 
    the code maintainability with the Delta Maintainability Model 
    since the goal of the paper is to improve security on the 
    maintainability.

\end{revcomment}

\Us Thank you for your comment. As we explained before, while 
addressing comment 1.2, both models assess maintainability.
Delta Maintainability Model is an alternative to our approach 
and only uses 5 guidelines of the set of guidelines our study considers.

\begin{revcomment}[1.5]

    In my opinion, the idea behind this work is very interesting, 
    but the research questions cannot be answered with the tools 
    and metrics selected. 

    My recommendation is to select a specific static analysis tool 
    for security issues and to include a maintainability model to 
    evaluate the maintenance. 

\end{revcomment}

\Us Thank you for the positive interest on the work. As we 
explained before Better Code Hub calculates maintainability metrics.
Our goal was to measure the impact of security patches on software 
maintainability. For future research, it would be interesting to 
understand if the results of these metrics can actually help 
security engineers assess the risk of their patches and guide 
them on performing better patches. However, in this work, 
we only focus on assessing software maintainability.

\newreviewer{2}

\begin{revcomment}[2.1]

    Question:  Is your dataset available?

\end{revcomment}

\Us Thank you for your question. The dataset is fully available at 
\texttt{dataset/db\_release\_security\_fixes.csv} in the figshare package
provided in the contributions of the paper: 
\url{https://figshare.com/s/4861207064900dfb3372}. If the paper is 
accepted, the package will be available on GitHub.

\begin{revcomment}[2.2]

    Abstract:
    Throughout the paper, "hypothesize" is probably a better word than 
    "suspect" for sounding more scientific.
    
    Your results should be more specific than "show evidence of trace-off". 
    Briefly tell us about what metrics you used and what the numerical results 
    indicate.
    
\end{revcomment}

\Us We thank the reviewer for the suggestions. We addressed
both of the issues in the abstract. For the later issue, we reported 
the overall result for maintainability and results
for the two of the guidelines with more negative impact on software 
maintainability: software complexity and unit size.

\begin{revcomment}[2.3]

    Intro:
    \begin{itemize}
        \item First sentence - quality is not ONLY related to cost but also to security 
        and safety.
        \item Provide a URL to Software Improvement Group and Better Code Hub.
        \item Page 3, Line 1 - Application Security Verification Standard (ASVS).
        \item Page 3, Line 7 - instead of a "broad number of code metrics" - tell 
        us exactly the number of metrics.
        \item Page 3, Line 13 - "suggest" sounds more scientific than "hint at"
        \item Page 3, Line 24 - "we intend to highlight the need …". Is that the 
        broad goal of your paper?  The goal should be explicitly stated in the 
        abstract and intro.
    \end{itemize}
\end{revcomment}

\Us Thank you for reporting these issues. All of them were addressed in 
the new version of the paper. In the last point, where the reviewer asks ``Is that the 
broad goal of your paper?'', we want to clarify that the goal of our study
is to show the impact of security patches on software maintainability and
highlight the need for solutions which are described in the end of 
the abstract, Section 1 (\textit{Introduction}) and Section 5 (\textit{Study Implications}).

\begin{revcomment}[2.4]

    Intro:\\
    You should check out this paper: 
    Li, Frank and Paxson, Vern. A large-scale empirical study of security patches.
    ACM SIGSAC Conference on Computer and Communications Security in 2017.

\end{revcomment}

\Us Thank you for bringing our attention to this paper. We integrated
this paper in the Related Work section and used it to support some 
of our claims and findings.

\begin{revcomment}[2.5]

    Section 2\\
    \begin{itemize}
        \item The first paragraph is redundant with Section 1.  
        \item Page 4, Line 15 - You define the OCSP acronym late in the paragraph.
        \item Page 5, Line 6 - how many new branch points?
        \item Page 6, Line 19 - I think you want to say "In this study, maintainability is …."  
        Since it currently implies this information is available in the CWE.
        \item Page 6, line 47 - "regular commits" non-security commits to be more clear; maybe a 
        few more words to explain "randomly collected."  You also call them "baseline commits" in Figure 
        1 which gives a different phrase for the same concept.  Pick one and use it everywhere.
    \end{itemize}
\end{revcomment}

\Us Thank you for pointing out all of these issues. We addressed all of them. We decided
to use regular changes/regular commits instead of baseline and non-security commits.

\begin{revcomment}[2.6]
    Section 3\\
    \begin{itemize}
        \item Sometime you say 1330 patches (e.g. page 7, line 16) and sometimes 1300 (e.g. page 8, line 11, 
        and the abstract).  In science it's better to say the exact number and use it always and not round.  
        \item You say the dataset had 1330 (or 1300) patches and 1282 commits though you also say one patch 
        can have multiple commits assigned.  So how did you end up with less than one commit/patch?
    \end{itemize}
\end{revcomment}

\Us Thank you for raising this concern. The right number of security patches is $1300$ ($624$ from Ponta et al. and 
$676$ from Secbench). The $1330$ value is a typo, we fixed the issue. Regarding the second point, $1282$ commits is
the equivalent to the $624$ patches that integrate the Pontas et al. dataset where one patch
can have multiple commits. The $1282$ commits are referring to the Ponta et al. dataset and 
not to the combined dataset.

\begin{revcomment}[2.7]
    Section 3\\
    \begin{itemize}
        \item I would really like a better picture of what SIG is to establish credibility.  I was at first 
        thinking it had something to do with ACM SIG's but now after going to the website, it seems like a 
        commercial/consulting organization? I also feel irritated that I don't understand what you really mean 
        by "running against the BCH toolset" (page 8, line 13) since you have repeatedly mentioned BCH like it was 
        an industry standard - but you have not told me if it's a static analysis tool or what it is.  The explanation 
        and link to introduce SIG and BCH should be on Page 2 lines 41-42 rather than Page 7 - though an explanation 
        of the BCH toolset is best a paragraph in the methodology.  
    \end{itemize}
\end{revcomment}

\Us Thank you for raising the concern regarding SIG's credibility. SIG is a company that 
has more than $20$ years of experience and research in software quality production. 
Their models are scientifically proven and certified. We added this information and 
the links the reviewer refer in the comment to the Introduction section.

\begin{quote}
    As ISO does not provide any specific guidelines/formulas to 
calculate maintainability, we resort to Software Improvement Group 
(SIG\footnote{SIG's website: 
\url{https://www.sig.eu/} (Accessed on \today{})})'s web-based source 
code analysis service Better Code Hub (BCH)\footnote{BCH's 
website: \url{https://bettercodehub.com/} (Accessed on \today{})}  
to compute the software compliance with a set of $10$ 
guidelines/metrics to produce quality software based in ISO/IEC 
$25010$ [Visser2016]. SIG 
has been helping business and technology leaders drive their organizational 
objectives by fundamentally improving the health and security of 
their software applications for more than 20 years. Their 
models are scientifically proven and certified [Alves2010, Alves2011, Baggen2012].
\end{quote}

We rephrased "running against the BCH toolset" to "analyzed using the BCH toolset". We mean
that we used BCH to calculate the metrics presented on Table 1. 

[Visser2016] J. Visser. Building Maintainable Software, Java Edition: Ten Guidelines 
for Future-Proof Code. O'Reilly Media, Inc. in 2016.

[Alves2010] T. L. Alves and C. Ypma and J. Visser. Deriving metric thresholds from 
benchmark data. IEEE International Conference on Software Maintenance in 2010.

[Alves2011] T. L. Alves and J. P. Correia and J. Visser. Benchmark-Based Aggregation 
of Metrics to Ratings. Joint Conference of the 21st International Workshop on Software 
Measurement and the 6th International Conference on Software Process and Product Measurement
in 2011.

[Baggen2012] R. Baggen, J. P. Correia, K. Schill, J. Visser. Standardized code quality 
benchmarking for improving software maintainability. Software Quality Journal in 2012.


\begin{revcomment}[2.8]
    Section 3\\
    \begin{itemize}
        \item Page 7, line 4 - I don't think "resemble" is the word you are looking for - but I don't 
        know what you are trying to say so I can't make a suggestion.
        \item Please explain your repeatable methodology to "clean the dataset and select the most relevant and 
        compatible projects" (page 8, line 14) that brings the patches from 1282 to 969.  From 969 to 866 patches - 
        you just could not classify?  Please explain.
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.9]
    Section 3\\
    \begin{itemize}
        \item Page 8, line 33 "regular commit" - pick baseline or regular (or non-security) commits and use it always.  
        I don't understand the sentence "The baseline dataset is generated from the security commits dataset."   
        Do you mean you extract these commits from the projects in the security commits dataset?
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.10]
    Section 3\\
    \begin{itemize}
        \item Section 3.2 is confusing enough that I'm not sure of the analysis - 
        I will need to review the revision that you would submit to gain confidence.  
        Are you saying you want your non-security commits (regular commits) to be about 
        the same size as a given security commit so you are using this criteria to choose 
        your regular commits?  Justify this reasoning and make the methodology clear.  
        I would have expected that you randomly collected 1300 non-security commits to compare 
        with the 1300 security commits. Why constrain to make them be of "similar" size?  
        Why not allow the characteristics of regular versus security commits to be what 
        they are without such curating?
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.11]
    Section 3\\
    \begin{itemize}
        \item Section 3.2 is confusing enough that I'm not sure of the analysis - 
        I will need to review the revision that you would submit to gain confidence.  
        Are you saying you want your non-security commits (regular commits) to be about 
        the same size as a given security commit so you are using this criteria to choose 
        your regular commits?  Justify this reasoning and make the methodology clear.  
        I would have expected that you randomly collected 1300 non-security commits to compare 
        with the 1300 security commits. Why constrain to make them be of "similar" size?  
        Why not allow the characteristics of regular versus security commits to be what 
        they are without such curating?
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.12]
    Section 3\\
    \begin{itemize}
        \item Since SIG seems to be "just another consulting organization" - I'd like 
        to see about the acceptance of the 10 guidelines in the software engineering 
        discipline.  I would guess you could find peer-reviewed references to support 
        these guidelines, or even books by Martin Fowler.   That has more credibility 
        than the reference by the SIG consulting group. For example on Page 9, line 42 
        you cite McCabe Complexity --  that has more credibility than a consulting 
        organizations guidelines that they use to make money on their tool.  Tell us more 
        about the BCH data experience since that is your comparison point for compliance.
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.13]
    Section 3\\
    \begin{itemize}
        \item Since SIG seems to be "just another consulting organization" - I'd like 
        to see about the acceptance of the 10 guidelines in the software engineering 
        discipline.  I would guess you could find peer-reviewed references to support 
        these guidelines, or even books by Martin Fowler.   That has more credibility 
        than the reference by the SIG consulting group. For example on Page 9, line 42 
        you cite McCabe Complexity --  that has more credibility than a consulting 
        organizations guidelines that they use to make money on their tool.  Tell us more 
        about the BCH data experience since that is your comparison point for compliance.
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.13]
    Section 3\\
    \begin{itemize}
        \item Page 10, line 49 "mainly single-commit patches" … "small percentage 
        of data points" - give us the exact numbers for each of these.  Page 11, 
        line 38 - define "floss refactoring" and justify how these were objectively 
        and repeatably identified in your set.   You say "these cases" which makes 
        it seems that you were including the set of patches that had more than one 
        commit as your (only) floss candidates.  
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.14]
    Section 3\\
    \begin{itemize}
        \item Page 11, Line 42 - I think you are saying you removed these two guidelines 
        from the analysis of all projects (as shown in Figure 4) but this paragraph seems 
        to say this was only for projects with large code bases.
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.15]
    Section 3\\
    \begin{itemize}
        \item Page 11  line 48.  How many were disregarded?
    \end{itemize}
\end{revcomment}

\Us until here

\begin{revcomment}[2.16]
    Section 4\\
    \begin{itemize}
        \item Page 13, Line 3 - 969 security commits and 969 
        baseline commits?  It's hard to know how to parse that.
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.17]
    Section 4\\
    \begin{itemize}
        \item Page 13, Line 31 "overall patches …" rather than having us 
        rely on looking at the Figure, tell us the number or percentage of 
        when the impact is positive like you do with the negative in the next 
        sentence. [You do provide the specifics starting on page 15 line 49.  
        I wanted it here.]
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.18]
    Section 4\\
    \begin{itemize}
        \item Page 19, Line 7 - for the unindoctrinated - I'd 
        explain the concept of Research Concepts.  Also explain 
        your choice of 7a and 7b and how representative they are 
        from the several hundred choices. Similar for your explanations 
        in the second two paragraphs on this page.  How far down the 
        concept/vulnerability hierarchy tree are these CWE choices or 
        are they lower level vulnerabilities chosen from the 700+ CWE types?
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.18]
    Section 4\\
    \begin{itemize}
        \item Page 20, Line 42 - how many is "a considerable number of cases"?  
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.18]
    Section 4\\
    \begin{itemize}
        \item Page 20, Line 45 - it seems choosing commits of similar size 
        to the security commits ended up causing a limitation.  I'm still 
        wondering why you chose that methodology anyway.  
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.19]
    Section 4\\
    \begin{itemize}
        \item RQ3 is answered to succinctly and with too little 
        detail to be convincing to me that the differences between 
        security and regular changes is different.  
    \end{itemize}
\end{revcomment}

\Us


\begin{revcomment}[2.20]
    Section 5\\
    \begin{itemize}
        \item Line 32 - since RQ3 was not convincing, 
        I don't feel a separate effort for maintainable security 
        is justified as separate from general focus on maintainability. 
    \end{itemize}
\end{revcomment}

\Us

\begin{revcomment}[2.20]
    Section 6\\
    \begin{itemize}
        \item It might not be a threat - but I feel the curation 
        of the non-security commits is a limitation to the generalizability 
        of the characteristics of non-security commits
    \end{itemize}
\end{revcomment}

\Us

\newreviewer{3}

\begin{revcomment}[3.1]

    The paper addresses a relevant and up to date topic. 
    The authors have analyzed a large set of project wrt to 
    quality assessment focused on vulnerabilities in line with 
    ISO25000 standard. There are some points worth considering: 
    - authors should better motivate the choice for BCH as web 
    based source code analysis service with respect to other 
    options that are available in literature. Tools such as Kiuwan, 
    sonar cloud, codify etc are also able to provide specific metrics 
    on maintainability and security, vulnerability features of code. 
    Furthermore Kiuwan is also an example designed based on the ISO25000 
    standard. Authors discuss the need for tools for risk assessment in 
    the study implications (section5) but fail to assess why their choice 
    falls on BCH. Furthermore all tools provide different conceptualization 
    and evaluation criteria for calculating quality characteristics such as Vulnerabilities 
    and Security. Authors should also explicit how these quality characteristics are measured in BCH. 

\end{revcomment}

\Us 

\begin{revcomment}[3.2]

    In section 3a better state/formulate the hypothesis authors 
    analyze wrt to the Research questions in previous sections. 
    In general the empirical study is not structured according to any 
    of the guidelines or explicit states any of them. 

\end{revcomment}

\Us

\begin{revcomment}[3.3]

    The findings of the research should better emphasize what the 
    lessons learned and what better practices should be put in place 
    by developers to assure better quality of software. The take away 
    message remains hindered. 

\end{revcomment}

\Us


\end{document}